# Graph Phases 3-5: Design Planning Guide

> **Purpose:** This document guides future sessions through designing and implementing Phases 3-5 of the graph-based code intelligence system. Each phase should be fully designed before implementation begins.

## Overview

| Phase | Goal | Prerequisite | Output |
|-------|------|--------------|--------|
| **3** | Generate architecture documentation from graph | Phase 2 complete | `*-phase-3-implementation.md` |
| **4** | Graph-augmented Q&A retrieval | Phase 3 complete | `*-phase-4-implementation.md` |
| **5** | Iterative retrieval (CGRAG) | Phase 4 complete | `*-phase-5-implementation.md` |

## How to Use This Document

When starting a new session to design a phase:

1. Tell Claude: "Read `docs/plans/2026-01-17-graph-phases-3-5-planning.md` and let's design Phase N"
2. Claude will use the **brainstorming** skill to explore the design space
3. The session produces a design document: `docs/plans/YYYY-MM-DD-graph-phase-N-design.md`
4. After design approval, create an implementation plan: `docs/plans/YYYY-MM-DD-graph-phase-N-implementation.md`
5. Commit both documents before starting implementation

---

## Phase 3: Architecture Documentation Generation

### Goal

Use the code graph to generate architecture documentation that traces real cross-cutting flows, replacing LLM-imagined diagrams with deterministic, graph-derived visualizations.

### Context

The current architecture page is generated by asking an LLM to synthesize information from file and directory summaries. This leads to "handwavy" diagrams that look plausible but don't reflect actual code structure. Phase 2 gives us a queryable graph of real relationships. Phase 3 uses this graph to:

1. Identify architectural subsystems
2. Detect important entry points and flows
3. Generate accurate Mermaid diagrams from graph traversals
4. Write narrative that explains what the graph shows

### Key Decisions to Make

These questions must be answered during the design session:

#### 1. What pages to generate?

Options:
- **Single "Architecture Overview" page** - One comprehensive page with multiple diagrams
- **Per-subsystem pages** - Separate pages for auth, API, database, etc.
- **Flow-specific pages** - "How Authentication Works", "Request Lifecycle"
- **Hybrid** - Overview + detailed subsystem pages

Consider:
- How does this integrate with existing wiki structure?
- What navigation changes are needed?
- How do we avoid duplicating content already in directory pages?

#### 2. How to identify subsystems?

Options:
- **Directory-based** - Each top-level directory is a subsystem
- **Graph clustering** - Use Leiden/Louvain algorithm to find densely connected components
- **Manual configuration** - User defines subsystems in `.oyaconfig`
- **Entry-point based** - Trace from each entry point to find its subgraph
- **Hybrid** - Start with directories, refine with graph analysis

Consider:
- What works for small vs large codebases?
- How do we handle cross-cutting concerns (logging, auth)?
- Should users be able to override automatic detection?

#### 3. How to detect "important" entry points?

Options:
- **High fan-in** - Nodes called by many others
- **Route handlers** - Functions with `@route`, `@app.get`, etc. decorators
- **No incoming calls** - Top-level functions that initiate flows
- **Name patterns** - `main`, `handle_*`, `process_*`
- **Configuration** - User-defined entry points

Consider:
- Different codebases have different patterns
- Framework-specific detection (FastAPI, Express, Spring)
- Should we detect multiple "layers" of importance?

#### 4. How much LLM involvement?

Options:
- **Diagram-only from graph** - LLM writes narrative, diagrams are pure graph
- **LLM-assisted layout** - LLM helps organize complex diagrams
- **LLM summarization** - Graph provides facts, LLM synthesizes prose
- **Minimal LLM** - Template-based narrative with variable substitution

Consider:
- Reproducibility (same input = same output)
- Cost (LLM calls per generation)
- Quality of narrative vs accuracy of structure

#### 5. How to handle low-confidence edges?

Options:
- **Omit entirely** - Only show high-confidence (>0.8) relationships
- **Visual distinction** - Dashed lines for uncertain relationships
- **Threshold per context** - Stricter for diagrams, lenient for narrative
- **Aggregate confidence** - Path confidence = product of edge confidences

Consider:
- What confidence threshold feels "right"?
- Should users be able to configure this?
- How do we communicate uncertainty to readers?

### Relevant Prior Work

- `docs/plans/2026-01-16-graph-architecture-design.md` - Original design decisions
- `docs/notes/improving-oya.md` - Section 7.2 on deterministic diagram generation
- `backend/src/oya/generation/prompts.py` - Current architecture generation prompts
- `backend/src/oya/generation/synthesis.py` - Current synthesis approach

### Design Session Checklist

Before finalizing the Phase 3 design:

- [ ] Decided on page structure (single vs multiple)
- [ ] Defined subsystem identification approach
- [ ] Specified entry point detection criteria
- [ ] Determined LLM involvement level
- [ ] Set confidence thresholds for diagrams
- [ ] Outlined integration with existing generation pipeline
- [ ] Identified changes to frontend (if any)

### Output

Create two documents:
1. `docs/plans/YYYY-MM-DD-graph-phase-3-design.md` - Design decisions and rationale
2. `docs/plans/YYYY-MM-DD-graph-phase-3-implementation.md` - TDD task list

### Continuation Instructions

After Phase 3 implementation is complete:

> "Phase 3 is complete. Read `docs/plans/2026-01-17-graph-phases-3-5-planning.md` and let's design Phase 4 (Graph-Augmented Q&A Retrieval)."

---

## Phase 4: Graph-Augmented Q&A Retrieval

### Goal

Combine vector search (semantic relevance) with graph traversal (structural connections) to answer questions with connected code context, not isolated chunks.

### Context

Current Q&A retrieval uses vector similarity to find relevant wiki pages/code. This misses structural relationships - if a user asks "how does auth flow to database?", we might find files mentioning "auth" and "database" but miss the intermediate service layer that connects them.

Phase 4 enhances retrieval by:
1. Using vector search to find entry points
2. Traversing the graph to find connected code
3. Presenting a coherent subgraph to the LLM

### Key Decisions to Make

#### 1. Retrieval strategy?

Options:
- **Vector-first, graph-expand** - Find top-K via vectors, expand each via graph
- **Graph-first, vector-rank** - Query graph for structure, rank by vector similarity
- **Parallel and merge** - Run both, combine and deduplicate
- **Adaptive** - Choose strategy based on query type

Consider:
- Query types: architectural ("how does X work?") vs point ("where is X defined?")
- Performance implications of each approach
- How to detect which strategy to use

#### 2. How many hops?

Options:
- **Fixed depth** - Always 2 hops
- **Dynamic based on query** - More hops for broad questions
- **Budget-based** - Expand until token budget approached
- **User-configurable** - Let user specify exploration depth

Consider:
- More hops = more context = better answers (to a point)
- More hops = higher cost and latency
- Some queries need depth, others need breadth

#### 3. Confidence threshold for Q&A?

Options:
- **Same as generation** - Use consistent threshold everywhere
- **Lower for Q&A** - Include uncertain paths, let LLM evaluate
- **Tiered** - High-confidence for direct answer, low for "related"
- **None** - Include all edges, note confidence in context

Consider:
- Low-confidence edges might still help LLM understand
- Too many low-confidence edges = noise
- Transparency: should we tell the LLM about confidence?

#### 4. Context budget management?

Options:
- **Hard cutoff** - Stop at N tokens
- **Prioritized selection** - Most relevant/central nodes first
- **Summarization** - Summarize distant nodes, include close ones verbatim
- **Code + summary hybrid** - Include code for key nodes, summaries for context

Consider:
- Graph can return a lot of code quickly
- Not all code is equally relevant
- How to prioritize when budget is tight

#### 5. How to present the subgraph to the LLM?

Options:
- **Flat code dump** - Just the code, no structure
- **Annotated sections** - Code with relationship comments
- **Structured context** - Use `Subgraph.to_context()` format
- **Include Mermaid** - Add diagram to help LLM understand
- **Hierarchical** - Organize by subsystem/file/function

Consider:
- LLMs can understand structure if presented clearly
- Mermaid diagrams use tokens but convey relationships efficiently
- Too much metadata = less room for code

### Relevant Prior Work

- `docs/plans/2026-01-13-qa-redesign-design.md` - Current Q&A architecture
- `docs/plans/2026-01-15-rag-indexing-improvements-design.md` - Indexing approach
- `backend/src/oya/api/routers/qa.py` - Current Q&A endpoint
- `backend/src/oya/vectorstore/` - Vector retrieval implementation
- `docs/notes/improving-oya.md` - Section 3.2 on GraphRAG

### Design Session Checklist

Before finalizing the Phase 4 design:

- [ ] Decided on retrieval strategy
- [ ] Defined hop depth approach
- [ ] Set confidence thresholds
- [ ] Designed context budget management
- [ ] Specified subgraph presentation format
- [ ] Identified changes to Q&A API
- [ ] Considered query classification (if adaptive)

### Output

Create two documents:
1. `docs/plans/YYYY-MM-DD-graph-phase-4-design.md` - Design decisions and rationale
2. `docs/plans/YYYY-MM-DD-graph-phase-4-implementation.md` - TDD task list

### Continuation Instructions

After Phase 4 implementation is complete:

> "Phase 4 is complete. Read `docs/plans/2026-01-17-graph-phases-3-5-planning.md` and let's design Phase 5 (Iterative Retrieval / CGRAG)."

---

## Phase 5: Iterative Retrieval (CGRAG)

### Goal

Implement Contextually-Guided RAG where the LLM identifies gaps in its context ("I see `verify_user` called but don't have its definition") and the system fetches missing pieces in subsequent passes.

### Context

Even with graph-augmented retrieval, the first pass might miss relevant code. CGRAG (from the improving-oya.md document) mimics how developers read code - following references iteratively until understanding is complete.

Phase 5 adds:
1. Gap detection - LLM identifies what's missing
2. Targeted retrieval - System fetches specific missing pieces
3. Iterative refinement - Multiple passes until sufficient

### Key Decisions to Make

#### 1. How many iterations?

Options:
- **Fixed** - Always 2 passes
- **LLM-controlled** - Until LLM says "sufficient"
- **Budget-based** - Stop at cost/latency limit
- **Confidence-based** - Stop when answer confidence is high

Consider:
- More iterations = better answers = higher cost
- Diminishing returns after N iterations
- Need a guaranteed termination condition

#### 2. Gap detection method?

Options:
- **Explicit LLM prompt** - Ask "what information is missing?"
- **Parse for unresolved references** - Find mentions of undefined symbols
- **Confidence-based** - Low confidence = needs more context
- **Hybrid** - LLM + parsing

Consider:
- LLM detection is flexible but uses tokens
- Parsing is deterministic but might miss semantic gaps
- How to format gap requests for efficient retrieval

#### 3. What triggers CGRAG vs single-pass?

Options:
- **Always multi-pass** - Every query gets CGRAG
- **Query complexity** - Simple queries get one pass
- **Initial confidence** - Low confidence triggers iteration
- **User toggle** - Let user request deep analysis
- **Automatic detection** - System decides based on query type

Consider:
- Cost for simple queries
- User expectations and latency
- How to detect "complex" queries

#### 4. How to avoid loops?

Options:
- **Track fetched context** - Never fetch same content twice
- **Max iterations** - Hard limit (e.g., 3 passes)
- **Monotonic progress** - Require confidence increase each pass
- **Timeout** - Stop after N seconds

Consider:
- LLM might repeatedly ask for same thing differently
- Need graceful degradation if stuck
- User feedback if we give up

#### 5. Caching strategy?

Options:
- **No caching** - Every query is fresh
- **Session caching** - Within a conversation
- **Query similarity** - Cache similar query patterns
- **Pre-fetch popular paths** - Cache common traversals

Consider:
- Repeated questions in a session
- Storage costs
- Cache invalidation when code changes

### Relevant Prior Work

- `docs/notes/improving-oya.md` - Section 3.3 on CGRAG concept
- The dir-assistant paper/blog on CGRAG implementation
- `backend/src/oya/qa/` - Current Q&A implementation
- `backend/src/oya/constants/qa.py` - Token budgets and thresholds

### Design Session Checklist

Before finalizing the Phase 5 design:

- [ ] Decided on iteration limit approach
- [ ] Designed gap detection mechanism
- [ ] Defined CGRAG trigger conditions
- [ ] Specified loop avoidance strategy
- [ ] Considered caching approach
- [ ] Identified changes to Q&A service
- [ ] Designed user-facing behavior (loading, status)

### Output

Create two documents:
1. `docs/plans/YYYY-MM-DD-graph-phase-5-design.md` - Design decisions and rationale
2. `docs/plans/YYYY-MM-DD-graph-phase-5-implementation.md` - TDD task list

### Continuation Instructions

After Phase 5 implementation is complete, the graph-based code intelligence system is feature-complete. Consider:

1. **Benchmarking** - Measure accuracy improvements over naive RAG
2. **Performance optimization** - Profile and optimize hot paths
3. **User feedback collection** - Gather data on answer quality
4. **Documentation** - Update user-facing docs about new capabilities

---

## Appendix: Design Session Best Practices

### Before the Session

1. Read the relevant phase section in this document
2. Review the referenced prior work documents
3. Have the architecture design doc open: `docs/plans/2026-01-16-graph-architecture-design.md`

### During the Session

1. Use the **brainstorming** skill - one question at a time
2. Explore 2-3 approaches for each key decision
3. Document trade-offs explicitly
4. Reference existing code patterns where applicable

### After the Session

1. Write the design document with clear decisions
2. Create the implementation plan with TDD tasks
3. Commit both documents before starting implementation
4. Update this planning document if needed

### Anti-Patterns to Avoid

- Starting implementation before design is complete
- Skipping the design session "because it's obvious"
- Making decisions without exploring alternatives
- Forgetting to commit documentation
- Implementing multiple phases without checkpoints
