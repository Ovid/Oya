# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Ọya is a local-first, editable wiki generator for codebases. It uses LLMs to generate documentation from source code.

**Storage:** Wikis stored in `~/.oya/wikis/{local_path}/` with `source/` (git clone) and `meta/` (wiki artifacts) subdirectories

IMPORTANT: this is a generic repository analysis tool. DO NOT MAKE ASSUMPTIONS ABOUT THE CODEBASE BEING ANALYZED. For example, we often use Oya to analyze itself and you often create designs assuming the same tech stack as Oya. THIS IS AN ERROR. Do not make assumptions about the programming languages, frameworks, tools, etc. These must be discovered, not assumed.

## Development Commands

### Backend (Python/FastAPI)

```bash
cd backend
source .venv/bin/activate  # Uses existing venv
pip install -e ".[dev]"    # Install with dev dependencies

# Run server
uvicorn oya.main:app --reload

# Run tests
pytest                      # All tests
pytest tests/test_qa_api.py # Single file
pytest -k "test_name"       # By name pattern
```

**API Documentation (auto-generated by FastAPI):**
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
- OpenAPI JSON: http://localhost:8000/openapi.json

### Frontend (React/TypeScript/Vite)

```bash
cd frontend
npm install
npm run dev      # Dev server on :5173
npm run build    # TypeScript check + Vite build
npm run lint     # ESLint
npm run test     # Vitest (run once)
npm run test:watch  # Vitest (watch mode)
```

### Docker

```bash
docker-compose up  # Runs both services
```

## Architecture

### Backend Structure (`backend/src/oya/`)

- **api/routers/**: FastAPI endpoints (repos, repos_v2, wiki, jobs, search, qa, notes)
- **db/**: SQLite for job tracking, metadata, and repo registry
  - `repo_registry.py`: Multi-repo CRUD operations
- **generation/**: Wiki generation pipeline
  - `orchestrator.py`: Main generation coordinator - handles the full pipeline
  - `prompts.py`: All LLM prompt templates
  - `synthesis.py`: Combines parsed code into documentation
  - `summaries.py`: Hierarchical code summarization
  - `staging.py`: Atomic wiki updates via staging directory
- **llm/**: LiteLLM-based client supporting OpenAI, Anthropic, Google, Ollama
- **parsing/**: Tree-sitter based code parsers (Python, TypeScript, Java, fallback)
- **repo/**: Repository management
  - `url_parser.py`: Parse git URLs and detect source type
  - `git_operations.py`: Clone/pull wrappers with error handling
  - `repo_paths.py`: Directory structure utilities for multi-repo storage
- **vectorstore/**: ChromaDB for semantic search and Q&A
- **notes/**: Human correction system

### Frontend Structure (`frontend/src/`)

- **components/**: React components
  - `Layout.tsx`, `Sidebar.tsx`, `TopBar.tsx`: Shell UI
  - `RepoDropdown.tsx`: Repository selector with status indicators
  - `AddRepoModal.tsx`: Add new repository modal
  - `FirstRunWizard.tsx`: Welcome screen for first-time setup
  - `GenerationProgress.tsx`: Real-time job progress via SSE
  - `IndexingPreviewModal.tsx`: File selection before generation
  - `QADock.tsx`: Q&A interface
  - `pages/`: Route components (Overview, Architecture, Workflow, Directory, File)
- **stores/**: Zustand state management
  - `reposStore.ts`: Multi-repo state (list, active repo, CRUD)
  - `wikiStore.ts`: Wiki tree and page content
  - `generationStore.ts`: Job tracking and progress
  - `uiStore.ts`: UI state (panels, modals)
- **api/**: Typed API client functions

### Data Flow

1. User triggers generation → `POST /api/repos/init`
2. Backend creates job, starts async generation via `orchestrator.py`
3. Progress streamed via SSE at `/api/jobs/{id}/stream`
4. Wiki written to `.oyawiki-building/` (staging), then atomically promoted to `.oyawiki/`
5. Frontend polls `/api/wiki/tree` and renders markdown pages

### Key Patterns

- **Settings**: Loaded via `config.py:load_settings()` from env vars, cached with `@lru_cache`
- **LLM calls**: All go through `llm/client.py`, logged to `.oya-logs/llm-queries.jsonl`
- **Tests**: pytest with `asyncio_mode = "auto"`, hypothesis for property testing
- **Staging**: Generation writes to `.oyawiki-building/`, promotes atomically on success

### Configuration Constants

Hard-coded values that control application behavior are extracted to config files for easier tuning and documentation.

**Backend:** Configuration is centralized in `backend/src/oya/config.py` with the `CONFIG_SCHEMA` dictionary defining all settings with defaults and validation. Access via `load_settings().section.property`:
- `settings.generation` - LLM temperatures, chunking parameters
- `settings.ask` - Q&A token budgets, confidence thresholds, CGRAG settings
- `settings.llm` - Default LLM client settings
- `settings.search` - Result limits, deduplication
- `settings.files` - File size limits, concurrency
- `settings.paths` - Directory names

**Frontend:** `frontend/src/config/`
- `layout.ts` - Panel dimensions, z-index layers
- `qa.ts` - Confidence level colors
- `storage.ts` - localStorage keys
- `timing.ts` - Polling intervals, relative time thresholds

## Architectural Discipline

Before implementing new functionality:
1. Search for existing utilities/helpers that do similar things
2. Check if the pattern exists elsewhere in the codebase
3. If implementing something common (date formatting, error handling, API calls, validation), assume it already exists and search first

When modifying files:
- Imports go at module top, not inside functions
- If a class exceeds ~10 attributes or ~15 methods, consider splitting

### Before Writing "General Purpose" Code

When about to write something that feels reusable:
- Utility functions (formatting, parsing, validation)
- API/HTTP helpers
- Error handling patterns
- Data transformation logic
- UI components (buttons, modals, form elements)

**Stop and search first:**
1. Grep for similar function names or keywords
2. Check obvious locations (utils/, helpers/, common/, shared/, lib/)
3. Look at how similar features were implemented elsewhere in the codebase

If found: reuse or extend existing code.
If close but not quite: refactor existing rather than duplicate.

### Code Review: Architectural Checks

When reviewing code (your own or others'), check for these flaws:

**Import Placement**: Flag imports inside functions/methods. These should be at module top unless avoiding a circular import.

**God Object Detection**: If a class exceeds 10 attributes or 15 public methods, evaluate cohesion:
- Do all attributes/methods serve ONE clear responsibility?
- Would you struggle to describe the class's purpose in one sentence?
- Are there subsets of methods that only use subsets of attributes?
- Legitimate large classes: data containers, facades, protocol implementations, test fixtures
- Flag as issue only if incoherent, not merely large

**Duplication Review**: If similar code appears multiple times:
- Is this test code? (Often acceptable)
- Is this the second occurrence? (Wait for third before abstracting)
- Would abstracting create a confusing helper with too many parameters?
- Flag only if: 3+ occurrences AND abstraction would be cleaner

**Missing Reuse**: Check if new helpers duplicate existing infrastructure in the codebase.

## Environment Variables

- `OYA_DATA_DIR`: Directory for storage (default: `~/.oya`)

LLM config (auto-detected from available keys):
- `ACTIVE_PROVIDER`: openai | anthropic | google | ollama
- `ACTIVE_MODEL`: Model name (has provider-specific defaults)
- Provider API keys: `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`
- `OLLAMA_ENDPOINT`: Defaults to `http://localhost:11434`

## Code Style

- Backend: Python 3.11+, ruff for linting, line length 100
- Frontend: TypeScript strict, ESLint, Tailwind CSS

## Error Handling

### Never Silently Discard Errors
Errors that disappear make debugging impossible. Every error must either:
1. **Propagate** - Let it bubble up to a handler that can deal with it
2. **Log** - Record what went wrong with context (file, operation, relevant data)
3. **Transform** - Convert to a user-visible error state or message

### Catch Specific Exceptions
```python
# BAD - catches everything including bugs
except Exception:
    pass

# GOOD - catches what you expect
except (FileNotFoundError, PermissionError) as e:
    logger.warning(f"Could not read {path}: {e}")
```

### When Generic Except is Acceptable
Only in these cases, and MUST include a comment explaining why:
1. **Resource cleanup in finally/close()** - Best-effort cleanup where failure doesn't matter
2. **Graceful degradation** - Feature works without this, AND you log the fallback
3. **Top-level handlers** - API endpoints, CLI entry points that must not crash

### Required Documentation for `pass` in Except
If you must use `pass`, the comment must explain:
- What errors are expected
- Why ignoring them is safe
- What the fallback behavior is

```python
# ACCEPTABLE - documented, specific scenario
except sqlite3.OperationalError:
    # Column already exists from previous migration - safe to ignore
    pass

# UNACCEPTABLE - no explanation
except Exception:
    pass
```

### Distinguish "No Results" from "Query Failed"
Never return empty collections on error - this hides failures:
```python
# BAD - caller can't tell if search failed or found nothing
except Exception:
    return []

# GOOD - caller knows something went wrong
except ChromaDBError as e:
    logger.error(f"Vector search failed: {e}")
    raise SearchError(f"Search unavailable: {e}") from e
```